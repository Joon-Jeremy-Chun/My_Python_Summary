# Python Web Crawling
web 프로그램을 사용하게 해준다.

1. 크롤링: HTML, CSS, XML, JSON, API 의 web 데이터를 수집해온다.
2. 스크래이핑 : 테이터를 추출한다.(파싱)
3. 데이터 저장 
4. 그 뒤 분석, 시각화

#
## network 
전송 수신 통신

 HTTP 인터넷 통신을 위한 protocal
`http:`(기본 인터넷 프로토콜) `https:`(보안과 함께) `ftp:`(파일을 주고받을때)

1. 우리가 일반적으로 `http:` 에 주소를 침으로 `request()`함수를 서버에 보낸다.

2. 서버는 `responsor()`함수로 `HTM`L형식을 보낸다. 그걸 `application`은 해석해서 화면에 띄운다.

3. `HTML`의 내용중 필요부분을 추출한다.

HTML안에 CSS 형식을 구현 \
HTML안에 Javascript 기능 구현

#
## 크롤링과 그롤러
클롤러는 자동으로 웹 페이지에 있는 정보를 수집하는 프로르램

## 크롤링과 스크레이핑 할때 주의 사항
+ 웹사이트의 이용 규약을 확인한다.
+ robots.txt 와 robots메카 태그의 접근 제한 사항
+ 제한이 없더라도 서버에 부하가 가지 않게 속도 조절
+ `rel="nofollow"`가 설정돼 있으면 접근하닞 않든다.
+ 거부 조치가 있으면 즉시 멈춘다.

## 네트워크 요청
+ 간격 설정하기\
적어도 1초에 1번정도